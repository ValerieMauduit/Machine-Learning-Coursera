---
title: "Qualifying physical exercise"
author: "Valérie Mauduit"
date: "29 décembre 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(randomForest)
library(dplyr)
library(caret)
library(rattle)
```

# Introduction

Using electronic devices, we can know study how physical activity is done. In this set, 6 persons were asked to perform barbell lifts correctly and incorrectly in 5 different ways, using some of these devices. The goal of our study is to guess whereas exercises are correct or incorrect, knowing accelerometers data. Moreover, we would be able to detect which type of error people do on their exercises.

# Our data

We use data from PUC (Rio De Janeiro University). More information is available here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (section Weight Lifting Exercise Dataset).

``` {r loading data}
training <- read.table('pml-training.csv', 
                       sep = ',', header = TRUE)
testing <- read.table('pml-testing.csv', 
                      sep = ',', header = TRUE)
```

Data is already splitted in training and test sets, which contain `r dim(training)[1]` lines (training) and `r dim(testing)[1]` lines (testing).

First of all, some variables are not to be used in our study: 

* the ones with a large amount of NaN values, because theu correspond to bad records;
* the ones with near zero variation, because they are calculated from previous ones;
* the ones which deal with the test protocol: name of the user, time (people where asked to perform first one type of exercise, then an other, so time is directly relied with exercice type!) 

``` {r cleaning data}
nsv <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[ , which(nsv$nzv == FALSE)]
training <- training[, -(1:6)]
training <- training[, colSums(is.na(training)) < 1000]
```

From the first 160 variables, we keep `r dim(training)[2]`. The last one, named "classe", defines the exercice class: A is a correct exercise and B to E are different type of common errors.

Some previews allow us to see that "classe" depends one the variables recorded by electronic devices:
``` {r visualizing data, fig.height=5, fig.width=20, fig.align = "center", dpi = 72, fig.retina = 1}
featurePlot(x = select(training, contains("arm_")), 
            y = training$classe, 
            plot = "box")
qplot(pitch_forearm, 
      color = classe, 
      data = training, 
      geom = "density")
```

The box plots show that interquartile zones are different for class A or other classes exercises (for example: acc_arm_x is globally low, magnet_arm_z is globally high, comparing to other classes). The density plot shows that only class A exercises show low values of pitch_forearm.

## For cross-validation

As our we have many data (`r dim(training)[1]` lines), we can cut the training set to evaluate better out-of-sample error.

```{r cutting data}
partition <- createDataPartition(y =training$classe, 
                                 p = .75, 
                                 list = FALSE)
subset_train <- training[partition,]
subset_test  <- training[-partition,]
```

# Creating a model

To create a model, we will use the caret package, since it makes automatics some features: it chooses the best parameters for the choosen modelling. I also use cross validation with 5 folds. This means that the data is splitted in 5 samples, with equivalent repartition of “classe” values. And then, the simulation is done on a training set composed of four over five folds, and evaluated on the fifth. I can't use many trees for the random forest, due to an old computer (not enough memory or CPU.) 

I tune the number of randmoly selected predictors for each tree (mtry). We don't need to preprocess our data (like centering or scaling) because this has poor influence in tree classification.

``` {r modelling data, fig.height=10, fig.width=10, fig.align = "center", dpi = 72, fig.retina = 1}
ctrl <- trainControl(method = "cv",
                     number = 5,
                     classProbs = TRUE)
model <- train(classe ~ .,
               data = subset_train,
               method = "rf",
               tunelength = 5,
               ntree = 10,
               trControl = ctrl)
model$finalModel
plot(varImp(object=model),
     main="RF - Variable Importance")
```

We can see that the most important variables of the model are: 

* roll_belt
* pitch_forearm
* yaw_belt
* magnet_dumbbell_y
* pitch_belt
* magnet_dumbbell_z
* roll_forearm

## Out-of-sample error

The out-of-sample error is measured for each simulation case. The mean accuracy is provided in the model (up). The error is one minus accuracy: `r sprintf("%.3f", 1-mean(model$resample$Accuracy))`. We show below the density plot of all measured accuracies (twenty values).

```{r out-of-sample error, fig.height=2, fig.width=2, fig.align = "center", dpi = 72, fig.retina = 1}
qplot(model$resample$Accuracy, geom = "density")
```

We will use our "subset_test" dataframe to validate the out-of-sample error. 

```{r subset-test}
subset_test_pred <- predict(model, 
                            newdata = subset_test)
subset_test_real <- subset_test$classe
cm <- confusionMatrix(subset_test_pred,
                      subset_test_real)
cm
```

The measured out-of-sample error is `r sprintf("%.3f", 1-cm$overall[[1]])`, wich is very close to the value measured by caret-train.

# Using our model

We use the model on the testing set, that contains 20 data. We don't know the real "classe" values of these items so we can't verify our model on this sumilations.

```{r test prediction}
testingPred <- predict(model, newdata = testing)
testingPred
```

# Conclusion

The model allows to alert people in case of bad type of exercise, with an overall accuracy of `r sprintf("%.1f", cm$overall[[1]]*100)`%. Class A corresponds to a well-done exercise. Sensitivity for class A is the percentage of well-done exercises detected over all well-done exercises. Specificity for class A is the percentage of wrong-done exercises detected over all wrong-done exercises. They are all very close to 100%.