---
title: "Qualifying physical exercise"
author: "Valérie Mauduit"
date: "29 décembre 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(dplyr)
library(caret)
library(rattle)
```

# Introduction

Using electronic devices, we can know study how physical activity is done. In this set, 6 persons were asked to perform barbell lifts correctly and incorrectly in 5 different ways, using some of these devices. The goal of our study is to guess whereas exercises are correct or incorrect, knowing accelerometers data. Moreover, we would be able to detect which type of error people do on their exercises.

# Our data

We use data from PUC (Rio De Janeiro University). More information is available here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (section Weight Lifting Exercise Dataset).

``` {r loading data}
training <- read.table('pml-training.csv', 
                       sep = ',', header = TRUE)
testing <- read.table('pml-testing.csv', 
                      sep = ',', header = TRUE)

```

Data is already splitted in training and test sets, which contain `r dim(training)[1]` lines (training) and `r dim(testing)[1]` lines (testing).

First of all, some variables are not to be used in our study: 

* the ones with a large amount of NaN values, because theu correspond to bad records;
* the ones with near zero variation, because they are calculated from previous ones;
* the ones which deal with the test protocol: name of the user, time (people where asked to perform first one type of exercise, then an other, so time is directly relied with exercice type!) 

``` {r cleaning data}
nsv <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[ , which(nsv$nzv == FALSE)]
training <- training[, -(1:6)]
training <- training[, colSums(is.na(training)) < 1000]
```

From the first 160 variables, we keep `r dim(training)[2]`. The last one, named "classe", defines the exercice class: A is a correct exercise and B to E are different type of common errors.

Some previews allow us to see that "classe" depends one the variables recorded by electronic devices:
``` {r visualizing data, fig.height=4, fig.width=16, fig.align = "center"}
featurePlot(x = select(training, contains("arm_")), 
            y = training$classe, 
            plot = "box")
qplot(pitch_forearm, 
      color = classe, 
      data = training, 
      geom = "density")
```

# Creating a model

To create a model, we will use the caret package, since it makes automatics some features. We will use a multiple cross validation with 5 folders, made 4 times. This means that our data is splitted in 5 samples, with equivalent repartition of "classe" values. And then, the simulation is done on a training set composed of four over five foldes, and evaluated on the fifth. This is performed four times. This method allows us to evaluate the out-of-sample error, since each simulation is evaluated on a out-of-sample data folder.

Because of a low powerfull computer, I can't use a random forest modelling. I use a tree modelling "rpart" (Recursive Partitioning and Regression Tree), because we deal with a classification problem. This modelling will allow us to physically understand our modelling.

We tune the complexity parameter (cp) ten times, to obtain a better model. We don't need to preprocess our data (like centering or scaling) because this has poor influence in tree classification.

``` {r modelling data, fig.height=10, fig.width=20, fig.align = "center"}
ctrl <- trainControl(method = "repeatedcv",
                     number = 5,
                     repeats = 4,
                     classProbs = TRUE)
model <- train(classe ~ .,
               data = training,
               method = "rpart",
               tuneLength = 10,
               trControl = ctrl)
model
fancyRpartPlot(model$finalModel)
```

We can see that the most important variables of the model are: 

* roll_belt
* pitch_forearm
* magnet_dumbbell_y
* roll_forearm
* total_accel_dumbbell
* magnet_dumbbell_z

## Out-of-sample error

The out-of-sample error is measured for each simulation case. The mean accuracy is provided in the model (up). The error is one minus accuracy: `r sprintf("%.3f", 1-mean(model$resample$Accuracy))`. We show below the density plot of all measured accuracies (twenty values).

```{r out-of-sample error, fig.height=3, fig.width=3, fig.align = "center"}
qplot(model$resample$Accuracy, geom = "density")
```

We may obtain a better prediction using a PCA (Principal Component Analysis) but it would be at the expense of variable meaning.

# Using our model

We use the model on the testing set, that contains 20 data. We don't know the real "classe" values of these items.

```{r test prediction}
testingPred <- predict(model, newdata = testing)
testingPred
```

# Conclusion

```{r confusion matrix}
cm <- confusionMatrix(predict(model, data = training),
                training$classe)
cm
```

The model allows to alert people in case of bad type of exercise, with an overall accuracy of 68.6%. Class A corresponds to a well-done exercise. Sensitivity for class A is `r sprintf("%.1f", cm$byClass[1, 1]*100)`%. It is the percentage of well-done exercises detected over all well-done exercises. Specificity for class A is `r sprintf("%.1f", cm$byClass[1, 2]*100)`%. It is the percentage of wrong-done exercises detected over all wrong-done exercises. Specificity is better that sensitivity: we detect errors better than well-done exercises. Class A has the best sensitivity, which is good: we predict well-done exercises better than each type of error, one-by-one.